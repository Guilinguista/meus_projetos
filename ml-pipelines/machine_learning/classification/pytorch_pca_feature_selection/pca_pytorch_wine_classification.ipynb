{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CkkrYP40YyGs"
      },
      "outputs": [],
      "source": [
        "# Import standard Python libraries for data handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import scikit-learn tools for preprocessing, PCA, and train/test splitting\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import PyTorch libraries for building and training the model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# STEP 1: Load dataset from the UCI repository\n",
        "# ----------------------------------------\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
        "df = pd.read_csv(url, sep=';')  # Read CSV using semicolon separator"
      ],
      "metadata": {
        "id": "LB4Cw5BjY609"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# STEP 2: Separate features (X) and target (y)\n",
        "# ----------------------------------------\n",
        "X = df.drop(\"quality\", axis=1)   # Features: all columns except 'quality'\n",
        "y = df[\"quality\"]                # Target: wine quality score (3 to 8)\n",
        "\n",
        "# Convert to binary classification: good (>=6) vs bad (<6)\n",
        "y = (y >= 6).astype(int)         # Now y = 1 if quality >= 6, else 0"
      ],
      "metadata": {
        "id": "F8TqlIspY_JE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# STEP 3: Split into train and test sets\n",
        "# ----------------------------------------\n",
        "# test_size=0.2 → 20% of the data will be for testing, 80% for training\n",
        "# random_state=42 → ensures reproducibility; every run gives the same split\n",
        "# (Fun fact: 42 is a joke from the book \"The Hitchhiker’s Guide to the Galaxy\" where 42 is 'the answer to life, the universe, and everything.')\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "ympNBqEYZFtN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# STEP 4: Standardize features (important before PCA)\n",
        "# ----------------------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)   # Fit scaler on training data and transform it\n",
        "X_test_scaled = scaler.transform(X_test)         # Transform test data using the same scaler\n"
      ],
      "metadata": {
        "id": "5N1xJktVZHRk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# STEP 5: Apply PCA to reduce dimensionality\n",
        "# ----------------------------------------\n",
        "pca = PCA(n_components=5)                        # Keep only 5 principal components\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)  # Fit PCA on train and transform train data\n",
        "X_test_pca = pca.transform(X_test_scaled)        # Transform test data with the same PCA\n",
        "\n",
        "# Print how much variance each component explains\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQeLZRKYZKo2",
        "outputId": "859e9b4d-0e74-4cf2-b55b-628082919f26"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio: [0.2801769  0.17563995 0.13957636 0.11081822 0.09021248]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# STEP 6: Convert NumPy arrays to PyTorch tensors\n",
        "# ----------------------------------------\n",
        "X_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "# .unsqueeze(1) turns shape from (N,) to (N, 1) to match model output shape\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n"
      ],
      "metadata": {
        "id": "4X_rqoC_ZOgA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# STEP 7: Define a simple feedforward neural network\n",
        "# ----------------------------------------\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 16)     # Input layer: 5 features → 16 neurons\n",
        "        self.fc2 = nn.Linear(16, 1)     # Output layer: 16 → 1 output neuron\n",
        "        self.relu = nn.ReLU()           # Activation between layers\n",
        "        self.sigmoid = nn.Sigmoid()     # Output activation for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))      # Apply first layer + ReLU\n",
        "        x = self.sigmoid(self.fc2(x))   # Apply second layer + Sigmoid\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleMLP()"
      ],
      "metadata": {
        "id": "3CkxG4qfZVFX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# STEP 8: Define loss function and optimizer\n",
        "# ----------------------------------------\n",
        "criterion = nn.BCELoss()                          # Binary Cross Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer with learning rate 0.01\n",
        "\n",
        "# ----------------------------------------\n",
        "# STEP 9: Train the model\n",
        "# ----------------------------------------\n",
        "epochs = 50                                       # Number of times to go through the dataset\n",
        "for epoch in range(epochs):\n",
        "    model.train()                                 # Set model to training mode\n",
        "    optimizer.zero_grad()                         # Clear gradients from previous step\n",
        "\n",
        "    output = model(X_train_tensor)                # Forward pass\n",
        "    loss = criterion(output, y_train_tensor)      # Calculate loss\n",
        "    loss.backward()                               # Backpropagation\n",
        "    optimizer.step()                              # Update weights\n",
        "\n",
        "    # Print training loss every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2QEF53zZZLV",
        "outputId": "324a9675-1a85-4fe3-ed02-ed66bdf7a592"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50 | Loss: 0.5906\n",
            "Epoch 20/50 | Loss: 0.5461\n",
            "Epoch 30/50 | Loss: 0.5366\n",
            "Epoch 40/50 | Loss: 0.5258\n",
            "Epoch 50/50 | Loss: 0.5196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# STEP 10: Evaluate model on test data\n",
        "# ----------------------------------------\n",
        "model.eval()                                      # Set model to evaluation mode\n",
        "with torch.no_grad():                             # Disable gradient tracking\n",
        "    preds = model(X_test_tensor)                  # Forward pass on test data\n",
        "    preds_label = (preds >= 0.5).float()          # Convert probabilities to 0 or 1\n",
        "    acc = (preds_label == y_test_tensor).float().mean()  # Accuracy calculation\n",
        "    print(f\"Test Accuracy: {acc.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy6VcM8VZaaw",
        "outputId": "259d3a8d-0e92-4ad4-c094-2a019b9fa058"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7188\n"
          ]
        }
      ]
    }
  ]
}